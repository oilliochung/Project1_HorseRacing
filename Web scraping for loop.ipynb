{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vjpLZ80fWZx"
   },
   "source": [
    "1-Sep-2019 to 12-Jul-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fGv-UksLfWZz"
   },
   "outputs": [],
   "source": [
    "#Import relevant modules\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bB8KlMuYfWZ7"
   },
   "outputs": [],
   "source": [
    "#Initialize variable\n",
    "#In this program, only data of 2019/2020 season is scraped\n",
    "#We start scrapping with the race on 1 Sep 2019 and end at 12 Jul 2020\n",
    "URL_racing_res = 'https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate=2020/07/15&RaceNo=1'\n",
    "URL_date_suffix= re.search(r'\\d{4}/\\d{2}/\\d{2}',URL_racing_res).group(0)\n",
    "URL_race_no_suffix = re.search(r'\\d+$',URL_racing_res).group(0)\n",
    "internation_race_day = ['2019/09/29','2019/10/13','2019/10/19','2019/11/05','2019/11/10','2019/11/24','2019/12/22','2020/07/04','2020/02/01','2020/02/20','2020/03/07','2020/04/04','2020/04/11','2020/06/01','2020/06/06','2020/06/16','2020/06/17','2020/06/18','2020/06/19','2020/06/20','2020/06/28','2020/07/04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please change the path to the location of your chromedriver\n",
    "driver = webdriver.Chrome('/Users/Hei/Applications/chromedriver')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pref_df():\n",
    "    '''Create DataFrame from performance table data'''\n",
    "    \n",
    "    global soup\n",
    "    #Get #Get all <td> elements in html\n",
    "    list_performance = get_html_from_soup(soup)\n",
    "\n",
    "    #Number of columns of performance data table\n",
    "    num_of_columns=12\n",
    "    #Number of cells in performance data table\n",
    "    num_of_table_element=len(list_performance)\n",
    "    #Number of horse\n",
    "    num_of_horse = int(num_of_table_element/num_of_columns)\n",
    "    \n",
    "    #Create dictionary for storing perfotmance table data\n",
    "    list_columns = ['place','horse_no','horse','jockey','trainer','actual_weight',\\\n",
    "                 'declared_horse_weight','draw','lbw','running_position','finish_time',\\\n",
    "                 'win_odds']\n",
    "\n",
    "    #Load performance table data into dataframe\n",
    "    table = soup.find_all(class_=\"performance\")\n",
    "    table_data = [i.find_all('td') for i in table]\n",
    "    l = [i.text.strip() for i in table_data[0]]\n",
    "    table=[]\n",
    "    for i in range(1,int(len(l)/12)):\n",
    "        table.append(l[12*i:12*i+12])\n",
    "    df_table=pd.DataFrame(table,columns=list_columns)\n",
    "  \n",
    "    return df_table,num_of_horse\n",
    "\n",
    "def get_html_from_soup(soup):\n",
    "    #Get all <td> elements in html\n",
    "    data_performance=soup.find_all(class_=\"f_fs12\")[1:-2][0]\n",
    "    list_performance=data_performance.find_all('td')\n",
    "    return list_performance\n",
    "\n",
    "def create_race_info_df():\n",
    "    global soup,num_of_horse,URL_date_suffix\n",
    "    \n",
    "    table = soup.find_all(class_=\"race_tab\")\n",
    "    table_data = [i.find_all('td')for i in table]\n",
    "    l = \"\".join([i.text for i in table_data[0]])\n",
    "    \n",
    "    line = soup.find_all(class_=\"f_fl f_fs13\")\n",
    "    line = line[0].text\n",
    "    location=re.search(r'\\w+\\s{2,}(.+)$',line).group(1)\n",
    "    \n",
    "\n",
    "    d=[[re.findall(\"RACE\\s(\\d+)\", l)[0]+re.findall(r\"RACE.+(\\(\\d+\\))\", l)[0],re.search(r'\\)(.+)Going',l).group(1),\n",
    "      re.findall(\"Going \\:(FIRM|GOOD TO FIRM|GOOD|GOOD TO YIELDING|YIELDING|YIELDING TO SOFT|SOFT|HEAVY|GOOD TO SOFT|WET FAST|FAST|SLOW|WET SLOW|RAIN AFFECTED|NORMAL WATERING)\\w+\", l)[0],\n",
    "       re.findall(\"Course \\:(.+)HK\", l)[0],\n",
    "       re.findall(\"HK\\$ \\d+\\,\\d*\\,*\\d*\",l)[0],location,URL_date_suffix] for i in range(num_of_horse)]\n",
    "\n",
    "    df=pd.DataFrame(d)\n",
    "    df.columns=('race','class','going','turf','prize','location','date')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_data_df(performance,race_info):\n",
    "    '''Create a DataFrame containing performance and race info data'''\n",
    "    df_data=pd.concat([performance,race_info],axis=1)\n",
    "    return df_data\n",
    "\n",
    "def get_num_of_race():\n",
    "    '''Find out the number of image for each race'''\n",
    "    pattern_img = re.compile('.+src=\"/racing/info/StaticFile/Images/Racing/racecard_rt.+')\n",
    "    img_list = [ str(tag) for tag in soup.find_all('img') if pattern_img.match(str(tag)) ]\n",
    "    race_no = [int(re.search(r'Racing/racecard_rt_(\\d+)',str(img)).group(1)) for img in img_list]\n",
    "\n",
    "    return max(race_no)\n",
    "\n",
    "def get_next_race():\n",
    "    '''Update URL_date_suffix, URL_race_no_suffix and return the url of next race. If it is internation event, skip current day'''\n",
    "    global soup,URL_date_suffix,URL_race_no_suffix\n",
    "\n",
    "    #local event:\n",
    "    #get next race no. suffix in url\n",
    "    max_race = get_num_of_race()\n",
    "    if (int(URL_race_no_suffix)+1<=max_race):\n",
    "        URL_race_no_suffix = str(int(URL_race_no_suffix)+1)\n",
    "    else:\n",
    "        #get next date suffix in url\n",
    "        date = soup.find(class_=\"f_fs11\")\n",
    "        date_selection = date.text.replace('\\n',',')[1:-1].split(',')\n",
    "        cur_race_day = URL_date_suffix[-2:]+URL_date_suffix[4:8]+URL_date_suffix[:4]\n",
    "        next_race_day = date_selection[date_selection.index(cur_race_day)-1]\n",
    "        while (next_race_day[-4:]+next_race_day[2:6]+next_race_day[:2]) in internation_race_day:\n",
    "            cur_race_day=next_race_day\n",
    "            next_race_day = date_selection[date_selection.index(cur_race_day)-1]\n",
    "        URL_date_suffix = next_race_day[-4:]+next_race_day[2:6]+next_race_day[:2]\n",
    "        URL_race_no_suffix = str(1)\n",
    "    \n",
    "    next_race_url='https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate='+URL_date_suffix+'&RaceNo='+URL_race_no_suffix\n",
    "    return next_race_url\n",
    "\n",
    "def check_race():\n",
    "    '''Check whether the race is cancelled. Return True if the race is scheduled and False if the race is cancalled'''\n",
    "    perf_content = soup.find(class_=\"race_tab\")\n",
    "    return bool(perf_content)\n",
    "\n",
    "def isIntRace():\n",
    "    '''Return True if it is an international event. Otherwise return false.'''\n",
    "    global internation_race_day\n",
    "    return True if URL_date_suffix in internation_race_day else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty dataframe storing all data from the whole season\n",
    "df_output = pd.DataFrame(columns=['place','horse_no','horse','jockey','trainer','actual_weight',\\\n",
    "                 'declared_horse_weight','draw','lbw','running_position','finish_time',\\\n",
    "                 'win_odds','race','class','going','turf','prize','location','date'])\n",
    "while True:\n",
    "    #Scrape data using chromedriver\n",
    "    #If the code fail to fetch enough html, please extend the sleep time\n",
    "    driver.get(URL_racing_res)\n",
    "    time.sleep(5)\n",
    "    subhtml = driver.page_source\n",
    "    soup = BeautifulSoup(subhtml, 'html.parser')\n",
    "    \n",
    "    if isIntRace():\n",
    "        #Skip current page and go to next day\n",
    "        URL_racing_res=get_next_race()\n",
    "    else:\n",
    "        #Get html data only if the race is scheduled. Otherwise, skip the page.\n",
    "        if check_race():\n",
    "            #Create dataframe containing performance data\n",
    "            df_perf,num_of_horse = create_pref_df()\n",
    "            #Creat dataframe containing racing info\n",
    "            df_race_info=create_race_info_df()\n",
    "            #Creat dataframe for analysis by concatenating df_perf and df_race_info\n",
    "            df_data = create_data_df(df_perf,df_race_info)\n",
    "            #Append df_output with df_data\n",
    "            df_output=pd.concat([df_output,df_data],axis=0)\n",
    "\n",
    "        #Update the url for next race\n",
    "        if (URL_date_suffix=='2020/07/15') and (URL_race_no_suffix=='9'):\n",
    "            break\n",
    "        else:\n",
    "            URL_racing_res=get_next_race()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_date_suffix,URL_race_no_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_racing_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv('2020_0715_performance.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Web scraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
